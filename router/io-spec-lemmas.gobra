// Copyright 2022 ETH Zurich
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// +gobra

package router

import (
	"sync"
	"github.com/scionproto/scion/pkg/slayers/path"
	"github.com/scionproto/scion/pkg/slayers"
	io "verification/io"
	sl "github.com/scionproto/scion/verification/utils/slices"
	. "verification/utils/definitions"
)
// IO-helper lemmas
/*ghost
decreases
pure func hf_validHelper(ts io.IO_ainfo, uinfo set[io.IO_msgterm], hf io.IO_HF, asid io.IO_as) bool {
	return let inif := hf.InIF2 in
		let egif := hf.EgIF2 in
		let x := hf.HVF in
		let l := io.IO_msgterm(io.MsgTerm_L{
			seq[io.IO_msgterm]{
				io.IO_msgterm(io.MsgTerm_Num{ts}),
				io.if2term(inif),
				io.if2term(egif),
				io.IO_msgterm(io.MsgTerm_FS{uinfo})}}) in
		x == io.mac(io.macKey(io.asidToKey(asid)), l)
}

//TODO: prove and choose correct triggers
ghost
requires  0 <= offset
requires  0 <= currHFIdx && currHFIdx <= len(asid)
requires  offset + path.HopLen * len(asid) <= len(raw)
preserves  acc(sl.AbsSlice_Bytes(raw, 0, len(raw)), R55)
ensures	  res == hopFieldsConsDir(raw, offset, currHFIdx, beta, asid, ainfo)
ensures	  forall k int :: {res[k]} 0 <= k && k < len(res) ==>
	res[k].extr_asid() == asid[k + currHFIdx]
ensures len(res) > 0 ==> hvfSet(res[0]) == beta //remove
ensures	  forall k int :: {res[k]} 0 < k && k < len(res) ==>
	hvfSet(res[k]) == io.upd_uinfo(hvfSet(res[k-1]), res[k-1])
ensures	  forall k int :: {res[k]} 0 <= k && k < len(res) ==>
 	hf_validHelper(ainfo, hvfSet(res[k]), res[k], asid[k + currHFIdx])
decreases
func hopFieldsConsDirLemma(raw []byte, offset int, currHFIdx int, beta set[io.IO_msgterm], asid seq[io.IO_as], ainfo io.IO_ainfo) (res seq[io.IO_HF])

//TODO: prove and choose correct triggers
ghost
requires  0 <= offset
requires  -1 <= currHFIdx && currHFIdx < len(asid)
requires  offset + path.HopLen * currHFIdx + path.HopLen <= len(raw)
preserves  acc(sl.AbsSlice_Bytes(raw, 0, len(raw)), R55)
ensures	  res == hopFieldsNotConsDir(raw, offset, currHFIdx, beta, asid, ainfo)
ensures	  forall k int :: {res[k]} 0 <= k && k < len(res) ==>
	res[k].extr_asid() == asid[k]
ensures	  len(res) > 0 ==> hvfSet(res[len(res)-1]) == beta //remove
ensures	  forall k int :: {res[k]} 0 <= k && k+1 < len(res) ==>
	hvfSet(res[k]) == io.upd_uinfo(hvfSet(res[k+1]), res[k+1])
ensures	  forall k int :: {res[k]} 0 <= k && k < len(res) ==>
 	hf_validHelper(ainfo, hvfSet(res[k]), res[k], asid[k])
decreases
func hopFieldsNotConsDirLemma(raw []byte, offset int, currHFIdx int, beta set[io.IO_msgterm], asid seq[io.IO_as], ainfo io.IO_ainfo) (res seq[io.IO_HF])
*/

//TODO: prove
ghost
preserves dp.Valid()
preserves acc(sl.AbsSlice_Bytes(raw, 0, len(raw)), R55)
ensures absIO_val(dp, raw, ingressID).isIO_val_Pkt2 ==>
	validPktMetaHdr(raw) && absPkt(dp, raw) != none[io.IO_pkt2] &&
	absIO_val(dp, raw, ingressID).IO_val_Pkt2_2 == get(absPkt(dp, raw)) &&
	len(get(absPkt(dp, raw)).CurrSeg.Future) > 0
decreases
func absIO_valLemma(dp io.DataPlaneSpec, raw []byte, ingressID uint16)

//TOOD: How to make this function opaque?
ghost
// opaque
requires acc(p.scionLayer.Mem(ub), _)
requires acc(&p.d, _) && acc(p.d.Mem(), _)
requires acc(&p.ingressID, _)
decreases
pure func (p *scionPacketProcessor) DstIsLocalIngressID(ub []byte) bool {
	return (unfolding acc(p.scionLayer.Mem(ub), _) in
		(unfolding acc(p.scionLayer.HeaderMem(ub[slayers.CmnHdrLen:]), _) in
		p.scionLayer.DstIA) == (unfolding acc(p.d.Mem(), _) in p.d.localIA)) ==> p.ingressID != 0
}

ghost
opaque
requires len(oldPkt.CurrSeg.Future) > 0
ensures  len(newPkt.CurrSeg.Future) > 0
decreases
pure func AbsUpdateNonConsDirIngressSegID(oldPkt io.IO_pkt2) (newPkt io.IO_pkt2) {
	return io.IO_pkt2(
			io.IO_Packet2{
				io.establishGuardTraversedseg(oldPkt.CurrSeg, !oldPkt.CurrSeg.ConsDir),
				oldPkt.LeftSeg,
				oldPkt.MidSeg,
				oldPkt.RightSeg})
}

ghost
opaque
requires len(pkt.CurrSeg.Future) > 0
decreases
pure func AbsValidateIngressIDConstraint(pkt io.IO_pkt2, ingressID option[io.IO_ifs]) bool {
	return let currseg := pkt.CurrSeg in
		ingressID != none[io.IO_ifs] ==>
			ingressID == (currseg.ConsDir ? currseg.Future[0].InIF2 : currseg.Future[0].EgIF2)
}

ghost
opaque
requires dp.Valid()
requires len(pkt.CurrSeg.Future) > 0
decreases
pure func AbsVerifyCurrentMACConstraint(pkt io.IO_pkt2, dp io.DataPlaneSpec) bool {
	return let currseg := pkt.CurrSeg in
		let d := currseg.ConsDir in
		let ts := currseg.AInfo in
		let hf := currseg.Future[0] in
		let uinfo := currseg.UInfo in
		dp.hf_valid(d, ts, uinfo, hf)
}

// assume absUinfo(p.infofield.UInfo) == oldPkt.CurrSeg.Uinfo
// assume absHVF(p.hopfield.HVF) == oldPkt.CurrSeg.Future[0].HVF
// in verfiyCurrentMac I prove dp.hf_valid(p.infofield.ConsDir, p.infofield.AInfo, p.infofield.UInfo, p.hopfield)

// TODO: add comment why terminations is assumed
ghost
requires dp.Valid()
requires ingressID != none[io.IO_ifs]
requires len(oldPkt.CurrSeg.Future) > 0
requires ElemWitness(ioSharedArg.IBufY, ingressID, oldPkt)
requires dp.dp2_enter_guard(
			oldPkt,
			oldPkt.CurrSeg,
			io.establishGuardTraversedseg(oldPkt.CurrSeg, !oldPkt.CurrSeg.ConsDir),
			dp.Asid(),
			oldPkt.CurrSeg.Future[0],
			get(ingressID),
			oldPkt.CurrSeg.Future[1:])
requires dp.dp3s_forward(
				io.IO_pkt2(
					io.IO_Packet2{
						io.establishGuardTraversedseg(oldPkt.CurrSeg, !oldPkt.CurrSeg.ConsDir),
						oldPkt.LeftSeg,
						oldPkt.MidSeg,
						oldPkt.RightSeg}),
				newPkt,
				egressID)
preserves acc(ioLock.LockP(), _) && ioLock.LockInv() == SharedInv!< dp, ioSharedArg !>;
ensures ElemWitness(ioSharedArg.OBufY, egressID, newPkt)
decreases _
func AtomicEnter(oldPkt io.IO_pkt2, ingressID option[io.IO_ifs], newPkt io.IO_pkt2, egressID option[io.IO_ifs], ioLock *sync.Mutex, ioSharedArg SharedArg, dp io.DataPlaneSpec) {
    ghost ioLock.Lock()
	unfold SharedInv!< dp, ioSharedArg !>()

	t, s := *ioSharedArg.Place, *ioSharedArg.State

	ApplyElemWitness(s.ibuf, ioSharedArg.IBufY, ingressID, oldPkt)
    ghost pkt_internal := io.IO_val(io.IO_Internal_val1{
                oldPkt,
                get(ingressID),
                newPkt,
                egressID})


	assert dp.dp3s_iospec_bio3s_enter_guard(s, t, pkt_internal)
	unfold dp.dp3s_iospec_ordered(s, t)
	unfold dp.dp3s_iospec_bio3s_enter(s, t)

	tN := io.CBio_IN_bio3s_enter_T(t, pkt_internal)
	io.Enter(t, pkt_internal) //Event

	UpdateElemWitness(s.obuf, ioSharedArg.OBufY, egressID, newPkt)

	ghost *ioSharedArg.State = io.dp3s_add_obuf(s, egressID, newPkt)
	ghost *ioSharedArg.Place = tN

	fold SharedInv!< dp, ioSharedArg !>()
	ghost ioLock.Unlock()
}


ghost
requires dp.Valid()
requires ingressID != none[io.IO_ifs]
requires egressID == none[io.IO_ifs]
requires len(oldPkt.CurrSeg.Future) > 0
requires ElemWitness(ioSharedArg.IBufY, ingressID, oldPkt)
requires newPkt == AbsUpdateNonConsDirIngressSegID(oldPkt)
requires AbsValidateIngressIDConstraint(oldPkt, ingressID)
requires AbsVerifyCurrentMACConstraint(newPkt, dp)
preserves acc(ioLock.LockP(), _) && ioLock.LockInv() == SharedInv!< dp, ioSharedArg !>;
ensures dp.Valid()
ensures ElemWitness(ioSharedArg.OBufY, egressID, newPkt)
decreases
func LocalEnterBytes(oldPkt io.IO_pkt2, ingressID option[io.IO_ifs], newPkt io.IO_pkt2, egressID option[io.IO_ifs], ioLock *sync.Mutex, ioSharedArg SharedArg, dp io.DataPlaneSpec) {
	reveal AbsUpdateNonConsDirIngressSegID(oldPkt)
	reveal AbsValidateIngressIDConstraint(oldPkt, ingressID)
	reveal AbsVerifyCurrentMACConstraint(newPkt, dp)
	AtomicEnter(oldPkt, ingressID, newPkt, egressID, ioLock, ioSharedArg, dp)
}