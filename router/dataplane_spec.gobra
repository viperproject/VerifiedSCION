// Copyright 2022 ETH Zurich
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//   http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// +gobra

package router

import (
	"errors"
	"hash"
	"net"

	"github.com/google/gopacket"

	"github.com/scionproto/scion/pkg/addr"
	"github.com/scionproto/scion/pkg/scrypto"
	"github.com/scionproto/scion/pkg/slayers"
	"github.com/scionproto/scion/pkg/slayers/path"
	"github.com/scionproto/scion/pkg/slayers/path/scion"
	underlayconn "github.com/scionproto/scion/private/underlay/conn"

	. "github.com/scionproto/scion/verification/utils/definitions"
	sl "github.com/scionproto/scion/verification/utils/slices"
)

pred MutexInvariant(d *DataPlane) {
	// access to the field 'mtx' ommited
	acc(&d.external,          1/2) &&
	acc(&d.linkTypes,         1/2) &&
	acc(&d.neighborIAs,       1/2) &&
	acc(&d.internal,          1/2) &&
	acc(&d.internalIP,        1/2) &&
	acc(&d.internalNextHops,  1/2) &&
	acc(&d.svc,               1/2) &&
	acc(&d.macFactory,        1/2) &&
	acc(&d.bfdSessions,       1/2) &&
	acc(&d.localIA,           1/2) &&
	acc(&d.running,           1/2) &&
	acc(&d.Metrics,           1/2) &&
	acc(&d.forwardingMetrics, 1/2) &&
	acc(&d.key,               1/2) &&
	(d.external    != nil       ==> accBatchConn(d.external))                      &&
	(d.linkTypes   != nil       ==> acc(d.linkTypes, 1/2))                         &&
	(d.neighborIAs != nil       ==> acc(d.neighborIAs, 1/2))                       &&
	(d.internal != nil          ==> d.internal.Mem())                              &&
	(d.internalIP != nil        ==> d.internalIP.Mem())                            &&
	(d.internalNextHops != nil  ==> accAddr(d.internalNextHops))                   &&
	(d.svc != nil               ==> d.svc.Mem())                                   &&
	(d.macFactory != nil        ==> (
		acc(d.key, 1/2)                                   &&
		acc(sl.AbsSlice_Bytes(*d.key, 0, len(*d.key)), _) &&
		scrypto.ValidKeyForHash(*d.key)                   &&
		d.macFactory implements MacFactorySpec{d.key}))                            &&
	(d.bfdSessions != nil       ==> accBfdSession(d.bfdSessions))                  &&
	(d.Metrics != nil           ==> acc(d.Metrics.Mem(), _))                       &&
	// The following permissions are enough to call all methods needed in fields
	// of forwardingMetrics
	(d.forwardingMetrics != nil ==> accForwardingMetrics(d.forwardingMetrics))
}

pred accAddr(addrs map[uint16]*net.UDPAddr) {
	acc(addrs, 1/2) &&
	forall a *net.UDPAddr :: { a in range(addrs) } a in range(addrs) ==> acc(a.Mem(), _)
}

pred accBatchConn(batchConns map[uint16]BatchConn) {
	acc(batchConns, 1/2) &&
	forall b BatchConn :: { b in range(batchConns) }{ b.Mem() } b in range(batchConns) ==>
		b.Mem()
}

pred accBfdSession(bfdSessions map[uint16]bfdSession) {
	acc(bfdSessions, 1/2) &&
	(forall bfd bfdSession :: { bfd in range(bfdSessions) }{ bfd.Mem() } bfd in range(bfdSessions) ==>
		(bfd != nil && acc(bfd.Mem(), _)))
}

pred accForwardingMetrics(metrics map[uint16]forwardingMetrics) {
	acc(metrics, 1/2) &&
	forall id uint16 :: { metrics[id] } id in domain(metrics) ==> acc(forwardingMetricsMem(metrics[id], id), _)
}

pred forwardingMetricsMem(v forwardingMetrics, ignoredForInjectivity uint16) {
	v.InputBytesTotal.Mem()     &&
	v.OutputBytesTotal.Mem()    &&
	v.InputPacketsTotal.Mem()   &&
	v.OutputPacketsTotal.Mem()  &&
	v.DroppedPacketsTotal.Mem()
}

pred forwardingMetricsNonInjectiveMem(v forwardingMetrics) {
	v.InputBytesTotal.Mem()     &&
	v.OutputBytesTotal.Mem()    &&
	v.InputPacketsTotal.Mem()   &&
	v.OutputPacketsTotal.Mem()  &&
	v.DroppedPacketsTotal.Mem()
}

ghost
requires  acc(forwardingMetricsNonInjectiveMem(v), _)
ensures   acc(forwardingMetricsMem(v, id), _)
decreases
func liftForwardingMetricsNonInjectiveMem(v forwardingMetrics, id uint16) {
	unfold acc(forwardingMetricsNonInjectiveMem(v), _)
	fold acc(forwardingMetricsMem(v, id), _)
}

pred (p *scionPacketProcessor) initMem() {
	acc(&p.d)                                    &&
	acc(&p.ingressID)                            &&
	acc(&p.buffer)                               &&
	acc(&p.mac)                                  &&
	acc(p.scionLayer.NonInitMem())               &&
	p.scionLayer.PathPoolInitializedNonInitMem() &&
	acc(&p.hbhLayer)                             &&
	acc(&p.e2eLayer)                             &&
	acc(&p.lastLayer)                            &&
	acc(&p.path)                                 &&
	acc(&p.hopField)                             &&
	acc(&p.infoField)                            &&
	acc(&p.segmentChange)                        &&
	acc(&p.cachedMac)                            &&
	acc(&p.macBuffers)                           &&
	acc(&p.bfdLayer)
}

requires acc(key, _) && acc(sl.AbsSlice_Bytes(*key, 0, len(*key)), _)
requires scrypto.ValidKeyForHash(*key)
ensures  acc(key, _) && acc(sl.AbsSlice_Bytes(*key, 0, len(*key)), _)
ensures  res != nil && res.Mem()
decreases
func MacFactorySpec(ghost key *[]byte) (res hash.Hash)

// useful to deal with incompletnesses
pred hideLocalIA(p *addr.IA) {
	acc(p)
}

pred (err scmpError) ErrorMem() {
	err.Cause != nil ==> err.Cause.ErrorMem()
}

scmpError implements error

type offsetPair struct {
	start int
	end int
	isNil bool
}

ghost
pure
requires 0 <= n
ensures  len(res) == n
ensures  forall i int :: {res[i]} 0 <= i && i < len(res) ==> res[i] == offsetPair{}
decreases
func newOffsetPair(n int) (res seq[offsetPair])

ghost
requires acc(MutexInvariant!<d!>(), _)
ensures  acc(MutexInvariant(d), _)
decreases
func (d *DataPlane) SimplifyPermInv() {
	unfold acc(MutexInvariant!<d!>(), _)
	// This big assertion seems to be necessary for the fold to succeed :(
	assert acc(&d.external,       _) &&
		acc(&d.linkTypes,         _) &&
		acc(&d.neighborIAs,       _) &&
		acc(&d.internal,          _) &&
		acc(&d.internalIP,        _) &&
		acc(&d.internalNextHops,  _) &&
		acc(&d.svc,               _) &&
		acc(&d.macFactory,        _) &&
		acc(&d.bfdSessions,       _) &&
		acc(&d.localIA,           _) &&
		acc(&d.running,           _) &&
		acc(&d.Metrics,           _) &&
		acc(&d.forwardingMetrics, _) &&
		acc(&d.key,               _) &&
		(d.external    != nil       ==> acc(accBatchConn(d.external), _))     &&
		(d.linkTypes   != nil       ==> acc(d.linkTypes, _))                  &&
		(d.neighborIAs != nil       ==> acc(d.neighborIAs, _))                &&
		(d.internal != nil          ==> acc(d.internal.Mem(), _))             &&
		(d.internalIP != nil        ==> acc(d.internalIP.Mem(), _))           &&
		(d.internalNextHops != nil  ==> acc(accAddr(d.internalNextHops), _))  &&
		(d.svc != nil               ==> acc(d.svc.Mem(), _))                  &&
		(d.macFactory != nil        ==> (
			acc(d.key, _)                                     &&
			acc(sl.AbsSlice_Bytes(*d.key, 0, len(*d.key)), _) &&
			scrypto.ValidKeyForHash(*d.key)                   &&
			d.macFactory implements MacFactorySpec{d.key}))                   &&
		(d.bfdSessions != nil       ==> acc(accBfdSession(d.bfdSessions), _)) &&
		(d.Metrics != nil           ==> acc(d.Metrics.Mem(), _))              &&
		(d.forwardingMetrics != nil ==> acc(accForwardingMetrics(d.forwardingMetrics), _))
	fold acc(MutexInvariant(d), _)
}

ghost
requires acc(MutexInvariant(d), _)
ensures  acc(MutexInvariant!<d!>(), _)
decreases
func (d *DataPlane) ElaboratePermInv() {
	unfold acc(MutexInvariant(d), _)
	// This big assertion seems to be necessary for the fold to succeed :(
	assert acc(&d.external,       _) &&
		acc(&d.linkTypes,         _) &&
		acc(&d.neighborIAs,       _) &&
		acc(&d.internal,          _) &&
		acc(&d.internalIP,        _) &&
		acc(&d.internalNextHops,  _) &&
		acc(&d.svc,               _) &&
		acc(&d.macFactory,        _) &&
		acc(&d.bfdSessions,       _) &&
		acc(&d.localIA,           _) &&
		acc(&d.running,           _) &&
		acc(&d.Metrics,           _) &&
		acc(&d.forwardingMetrics, _) &&
		acc(&d.key,               _) &&
		(d.external    != nil       ==> acc(accBatchConn(d.external), _))     &&
		(d.linkTypes   != nil       ==> acc(d.linkTypes, _))                  &&
		(d.neighborIAs != nil       ==> acc(d.neighborIAs, _))                &&
		(d.internal != nil          ==> acc(d.internal.Mem(), _))             &&
		(d.internalIP != nil        ==> acc(d.internalIP.Mem(), _))           &&
		(d.internalNextHops != nil  ==> acc(accAddr(d.internalNextHops), _))  &&
		(d.svc != nil               ==> acc(d.svc.Mem(), _))                  &&
		(d.macFactory != nil        ==> (
			acc(d.key, _)                                     &&
			acc(sl.AbsSlice_Bytes(*d.key, 0, len(*d.key)), _) &&
			scrypto.ValidKeyForHash(*d.key)                   &&
			d.macFactory implements MacFactorySpec{d.key}))                   &&
		(d.bfdSessions != nil       ==> acc(accBfdSession(d.bfdSessions), _)) &&
		(d.Metrics != nil           ==> acc(d.Metrics.Mem(), _))              &&
		(d.forwardingMetrics != nil ==> acc(accForwardingMetrics(d.forwardingMetrics), _))
	fold acc(MutexInvariant!<d!>(), _)
}

/**** Acessor methods to avoid unfolding the Mem predicate of the dataplane so much ****/
ghost
requires acc(MutexInvariant(d), _)
ensures  acc(&d.internalNextHops, _)
ensures  d.internalNextHops != nil ==> acc(accAddr(d.internalNextHops), _)
decreases
func (d *DataPlane) getInternalNextHops() {
	unfold acc(MutexInvariant(d), _)
}

ghost
requires acc(MutexInvariant(d), _)
ensures  acc(&d.forwardingMetrics, _)
ensures  d.forwardingMetrics != nil ==> acc(accForwardingMetrics(d.forwardingMetrics), _)
decreases
func (d *DataPlane) getForwardingMetrics() {
	unfold acc(MutexInvariant(d), _)
}

ghost
requires acc(MutexInvariant(d), _)
// The termination of this method is assumed. The reason is that the termination
// proof generated by this method is, essentially, an unfold acc(MutexInvariant(d), _)
// followed by a fold of the same invariant with the same permission amount.
// As shown in methods SimplifyPermInv and ElaboratePermInv, Gobra requires intermediate
// assertions to make this verify, which is not easy to add here.
decreases _
pure func (d *DataPlane) getValForwardingMetrics() map[uint16]forwardingMetrics {
	return unfolding acc(MutexInvariant(d), _) in d.forwardingMetrics
}

ghost
requires acc(MutexInvariant(d), _)
// The termination of this method is assumed. The reason is that the termination
// proof generated by this method is, essentially, an unfold acc(MutexInvariant(d), _)
// followed by a fold of the same invariant with the same permission amount.
// As shown in methods SimplifyPermInv and ElaboratePermInv, Gobra requires intermediate
// assertions to make this verify, which is not easy to add here.
decreases _
pure func (d *DataPlane) getDomForwardingMetrics() set[uint16] {
	return unfolding acc(MutexInvariant(d), _) in
		d.forwardingMetrics == nil ?
			set[uint16]{} :
			(unfolding acc(accForwardingMetrics(d.forwardingMetrics), _) in
				domain(d.forwardingMetrics))
}

ghost
requires acc(MutexInvariant(d), _)
// The termination of this method is assumed. The reason is that the termination
// proof generated by this method is, essentially, an unfold acc(MutexInvariant(d), _)
// followed by a fold of the same invariant with the same permission amount.
// As shown in methods SimplifyPermInv and ElaboratePermInv, Gobra requires intermediate
// assertions to make this verify, which is not easy to add here.
decreases _
pure func (d *DataPlane) getDomExternal() set[uint16] {
	return unfolding acc(MutexInvariant(d), _) in
		d.external == nil ?
			set[uint16]{} :
			(unfolding acc(accBatchConn(d.external), _) in
				domain(d.external))
}

ghost
requires acc(MutexInvariant(d), _)
decreases
pure func (d *DataPlane) WellConfigured() bool {
	return d.getDomExternal() == d.getDomForwardingMetrics()
}

ghost
requires acc(MutexInvariant(d), _)
requires id in d.getDomForwardingMetrics()
ensures  acc(&d.forwardingMetrics, _)
ensures  acc(d.forwardingMetrics, _)
ensures  acc(forwardingMetricsMem(d.forwardingMetrics[id], id), _)
decreases
func (d *DataPlane) getForwardingMetricsMem(id uint16) {
	unfold acc(MutexInvariant(d), _)
	assert id in d.getDomForwardingMetrics()
	assert d.getDomForwardingMetrics() == (d.forwardingMetrics == nil ?
		set[uint16]{} :
		(unfolding acc(accForwardingMetrics(d.forwardingMetrics), _) in
				domain(d.forwardingMetrics)))
	assert id in (d.forwardingMetrics == nil ?
		set[uint16]{} :
		(unfolding acc(accForwardingMetrics(d.forwardingMetrics), _) in
				domain(d.forwardingMetrics)))
	unfold acc(accForwardingMetrics(d.forwardingMetrics), _)
	assert id in domain(d.forwardingMetrics)
}

ghost
requires acc(MutexInvariant(d), _)
ensures  acc(&d.external, _) && (d.external != nil ==> acc(accBatchConn(d.external), _))
decreases
func (d *DataPlane) getExternalMem() {
	unfold acc(MutexInvariant(d), _)
}

ghost
requires acc(MutexInvariant(d), _)
ensures  acc(&d.linkTypes, _) && (d.linkTypes != nil ==> acc(d.linkTypes, _))
decreases
func (d *DataPlane) getLinkTypesMem() {
	unfold acc(MutexInvariant(d), _)
}

ghost
requires acc(MutexInvariant(d), _)
ensures  acc(&d.localIA, _)
decreases
func (d *DataPlane) getLocalIA() {
	unfold acc(MutexInvariant(d), _)
}

ghost
requires acc(MutexInvariant(d), _)
ensures  acc(&d.neighborIAs, _) && (d.neighborIAs != nil ==> acc(d.neighborIAs, _))
decreases
func (d *DataPlane) getNeighborIAs() {
	unfold acc(MutexInvariant(d), _)
}

ghost
requires acc(MutexInvariant(d), _) && d.getValSvc() != nil
ensures  acc(&d.svc, _) && d.svc != nil && acc(d.svc.Mem(), _)
decreases
func (d *DataPlane) getSvcMem() {
	unfold acc(MutexInvariant(d), _)
}

ghost
requires acc(MutexInvariant(d), _)
// The termination of this method is assumed. The reason is that the termination
// proof generated by this method is, essentially, an unfold acc(MutexInvariant(d), _)
// followed by a fold of the same invariant with the same permission amount.
// As shown in methods SimplifyPermInv and ElaboratePermInv, Gobra requires intermediate
// assertions to make this verify, which is not easy to add here.
decreases _
pure func (d *DataPlane) getValSvc() *services {
	return unfolding acc(MutexInvariant(d), _) in d.svc
}

ghost
requires acc(MutexInvariant(d), _)
ensures  acc(&d.bfdSessions, _) && (d.bfdSessions != nil ==> acc(accBfdSession(d.bfdSessions), _))
decreases
func (d *DataPlane) getBfdSessionsMem() {
	unfold acc(MutexInvariant(d), _)
}

ghost
requires acc(MutexInvariant(d), _)
ensures  acc(&d.internal, _) && (d.internal != nil ==> acc(d.internal.Mem(), _))
decreases
func (d *DataPlane) getInternal() {
	unfold acc(MutexInvariant(d), _)
}

requires acc(MutexInvariant(d), _)
ensures  acc(&d.macFactory, _)
decreases
func (d *DataPlane) getMacFactoryMem() {
	unfold acc(MutexInvariant(d), _)
}

ghost
requires acc(MutexInvariant(d), _)
requires acc(&d.macFactory, _) && d.macFactory != nil
ensures  acc(&d.macFactory, _) && acc(&d.key, _) && acc(d.key, _)
ensures  acc(sl.AbsSlice_Bytes(*d.key, 0, len(*d.key)), _)
ensures  scrypto.ValidKeyForHash(*d.key)
ensures  d.macFactory implements MacFactorySpec{d.key}
decreases
func (d *DataPlane) getNewPacketProcessorFootprint() {
	unfold acc(MutexInvariant(d), _)
}

/**** End of acessor methods to avoid unfolding the Mem predicate of the dataplane so much ****/

/** Start of closure specs for the Run method **/
requires true
func readClosureSpec(ingressID uint16, rd BatchConn)

requires true
func closureSpec1(ifID uint16, c bfdSession)

requires true
func closureSpec2(i uint16, c BatchConn)

requires true
func closureSpec3(c BatchConn)
/** End of closure specs for the Run method **/

/** definitions used internally for the proof of Run **/
// TODO: maybe drop and use the invariant of Messages instead now that we have IsActive?

pred writeMsgInv(writeMsgs underlayconn.Messages) {
	len(writeMsgs) == 1 &&
	acc(&writeMsgs[0]) &&
	len(writeMsgs[0].Buffers) == 1 &&
	acc(&writeMsgs[0].Buffers[0]) &&
	// sl.AbsSlice_Bytes(writeMsgs[0].Buffers[0], 0, len(writeMsgs[0].Buffers[0])) &&
	sl.AbsSlice_Bytes(writeMsgs[0].OOB, 0, len(writeMsgs[0].OOB)) &&
	0 <= writeMsgs[0].N
}
/** end of definitions used internally for the proof of Run **/

/** ErrorCell **/
pred (s *scmpError) Mem() {
	acc(s)
}

ghost
preserves s.Mem() && s.CanSet(e)
ensures   s.Get() === e
decreases
func (s *scmpError) Set(e error) {
	unfold s.Mem()
	defer fold s.Mem()
	newErr := e.(scmpError)
	*s = newErr
}

ghost
pure
requires acc(s.Mem(), _)
decreases
func (s *scmpError) Get() error {
	return unfolding acc(s.Mem(), _) in *s
}

ghost
pure
decreases
func (s *scmpError) CanSet(e error) bool {
	return typeOf(e) == type[scmpError]
}

/** spec for newPacketProcessor **/

// initial state, established after allocating with newPacketProcessor.
pred (s* scionPacketProcessor) sInit() {
	acc(&s.d)                                    &&
	acc(&s.ingressID)                            &&
	acc(&s.buffer) && s.buffer != nil            &&
	s.buffer.Mem()                               &&
	acc(&s.mac) && s.mac != nil && s.mac.Mem()   &&
	s.scionLayer.NonInitMem()                    &&
	// The following is not necessary
	// s.scionLayer.PathPoolInitializedNonInitMem() &&
	s.hbhLayer.NonInitMem()                      &&
	s.e2eLayer.NonInitMem()                      &&
	acc(&s.lastLayer)                            &&
	acc(&s.path)                                 &&
	acc(&s.hopField)                             &&
	acc(&s.infoField)                            &&
	acc(&s.segmentChange)                        &&
	acc(&s.cachedMac)                            &&
	acc(&s.macBuffers)                           &&
	sl.AbsSlice_Bytes(s.macBuffers.scionInput, 0, len(s.macBuffers.scionInput)) &&
	s.bfdLayer.NonInitMem()                      &&
	acc(&s.srcAddr)                              &&
	acc(&s.rawPkt)
}

// each ghost method on *scionPacketProcessor has, in the name, the state in which it
// expects to find the packet processor. In the case below, the state `Init` is expected.
ghost
requires acc(s.sInit(), _)
decreases
pure func (s* scionPacketProcessor) sInitD() (res *DataPlane) {
	return unfolding acc(s.sInit(), _) in s.d
}

ghost
requires acc(s.sInit(), _)
decreases
pure func (s* scionPacketProcessor) sInitRawPkt() (res []byte) {
	return unfolding acc(s.sInit(), _) in s.rawPkt
}

ghost
requires acc(s.sInit(), _)
decreases
pure func (s* scionPacketProcessor) sInitPath() (res *scion.Raw) {
	return unfolding acc(s.sInit(), _) in s.path
}

ghost
requires acc(s.sInit(), _)
decreases
pure func (s* scionPacketProcessor) sInitHopField() (res path.HopField) {
	return unfolding acc(s.sInit(), _) in s.hopField
}

ghost
requires acc(s.sInit(), _)
decreases
pure func (s* scionPacketProcessor) sInitInfoField() (res path.InfoField) {
	return unfolding acc(s.sInit(), _) in s.infoField
}

ghost
requires acc(s.sInit(), _)
decreases
pure func (s* scionPacketProcessor) sInitSegmentChange() (res bool) {
	return unfolding acc(s.sInit(), _) in s.segmentChange
}

/** end spec for newPacketProcessor **/

/** **/

ghost
requires hasScionLayer ==> scionLayer.Mem(ubScionLayer)
requires hasHbhLayer   ==> hbhLayer.Mem(ubHbhLayer)
requires hasE2eLayer   ==> e2eLayer.Mem(ubE2eLayer)
ensures  hasScionLayer ==> scionLayer.NonInitMem()
ensures  hasHbhLayer   ==> hbhLayer.NonInitMem()
ensures  hasE2eLayer   ==> e2eLayer.NonInitMem()
decreases
func ResetDecodingLayers(
	scionLayer *slayers.SCION,
	hbhLayer *slayers.HopByHopExtnSkipper,
	e2eLayer *slayers.EndToEndExtnSkipper,
	ubScionLayer []byte,
	ubHbhLayer   []byte,
	ubE2eLayer   []byte,
	hasScionLayer bool,
	hasHbhLayer   bool,
	hasE2eLayer   bool,
) {
	if hasScionLayer {
		scionLayer.DowngradePerm(ubScionLayer)
	}
	if hasHbhLayer {
		hbhLayer.DowngradePerm(ubHbhLayer)
	}
	if hasE2eLayer {
		e2eLayer.DowngradePerm(ubE2eLayer)
	}
}

/** **/
pred (d *DataPlane) validResult(result processResult, addrAliasesPkt bool) {
	acc(MutexInvariant(d), _)    &&
	// EgressID
	(result.EgressID != 0 ==> result.EgressID in d.getDomForwardingMetrics()) &&
	// OutConn
	(result.OutConn != nil ==> acc(result.OutConn.Mem(), _)) &&
	// OutAddr
	(addrAliasesPkt  && result.OutAddr != nil ==> acc(result.OutAddr.Mem(), R15)) &&
	(!addrAliasesPkt && result.OutAddr != nil ==> acc(result.OutAddr.Mem(), _))
	// OutPkt moved out
}

ghost
requires acc(MutexInvariant(d), _) && d.WellConfigured()
requires id in d.getDomExternal()
ensures  acc(MutexInvariant(d), _)
ensures  id in d.getDomForwardingMetrics()
decreases
func (d *DataPlane) InDomainExternalInForwardingMetrics(id uint16) {

}

ghost
requires acc(MutexInvariant(d), _) && d.WellConfigured()
requires acc(&d.external, _) && acc(d.external, _)
requires id in domain(d.external)
ensures  acc(MutexInvariant(d), _)
ensures  id in d.getDomForwardingMetrics()
decreases
func (d *DataPlane) InDomainExternalInForwardingMetrics2(id uint16) {
	unfold acc(MutexInvariant(d), _)
	unfold acc(accBatchConn(d.external), _)
}