// Copyright 2022 ETH Zurich
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// +gobra

package router

import (
	"sync"
	"github.com/scionproto/scion/pkg/slayers/path"
	"github.com/scionproto/scion/pkg/slayers/path/scion"
	"github.com/scionproto/scion/pkg/slayers"
	"verification/dependencies/encoding/binary"
	io "verification/io"
	sl "github.com/scionproto/scion/verification/utils/slices"
	. "verification/utils/definitions"
)

ghost
preserves dp.Valid()
preserves acc(sl.AbsSlice_Bytes(raw, 0, len(raw)), R55)
ensures   slayers.ValidPktMetaHdr(raw) && slayers.IsSupportedPkt(raw) ==>
	absIO_val(dp, raw, ingressID).isIO_val_Pkt2 &&
	absIO_val(dp, raw, ingressID).IO_val_Pkt2_2 == absPkt(dp, raw) &&
	len(absPkt(dp, raw).CurrSeg.Future) > 0
decreases
func absIO_valLemma(dp io.DataPlaneSpec, raw []byte, ingressID uint16) {
	if(slayers.ValidPktMetaHdr(raw) && slayers.IsSupportedPkt(raw)){
		absIO := reveal absIO_val(dp, raw, ingressID)
		assert absIO.isIO_val_Pkt2
		assert absIO_val(dp, raw, ingressID).IO_val_Pkt2_2 == absPkt(dp, raw)
		absPktFutureLemma(dp, raw)
	}
}

ghost
requires dp.Valid()
requires acc(sl.AbsSlice_Bytes(raw, 0, len(raw)), R56)
requires slayers.ValidPktMetaHdr(raw)
ensures  dp.Valid()
ensures  acc(sl.AbsSlice_Bytes(raw, 0, len(raw)), R56)
ensures  slayers.ValidPktMetaHdr(raw)
ensures  len(absPkt(dp, raw).CurrSeg.Future) > 0
decreases
func absPktFutureLemma(dp io.DataPlaneSpec, raw []byte) {
	reveal slayers.ValidPktMetaHdr(raw)
	headerOffset := slayers.GetAddressOffset(raw)
	assert forall k int :: {&raw[headerOffset:headerOffset+scion.MetaLen][k]} 0 <= k && k < scion.MetaLen ==> &raw[headerOffset:headerOffset+scion.MetaLen][k] == &raw[headerOffset + k]
	hdr := (unfolding acc(sl.AbsSlice_Bytes(raw, 0, len(raw)), R56) in
			binary.BigEndian.Uint32(raw[headerOffset:headerOffset+scion.MetaLen]))
	metaHdr := scion.DecodedFrom(hdr)
	currINFIdx := int(metaHdr.CurrINF)
	currHFIdx := int(metaHdr.CurrHF)
	seg1Len := int(metaHdr.SegLen[0])
	seg2Len := int(metaHdr.SegLen[1])
	seg3Len := int(metaHdr.SegLen[2])
	segLen := scion.LengthOfCurrSeg(currHFIdx, seg1Len, seg2Len, seg3Len)
	prevSegLen := scion.LengthOfPrevSeg(currHFIdx, seg1Len, seg2Len, seg3Len)
	numINF := scion.NumInfoFields(seg1Len, seg2Len, seg3Len)
	offset := scion.HopFieldOffset(numINF, 0, headerOffset)
	pkt := reveal absPkt(dp, raw)
	assert pkt.CurrSeg == reveal scion.CurrSeg(raw, offset+prevSegLen, currINFIdx, currHFIdx-prevSegLen, segLen, headerOffset)
	assert len(pkt.CurrSeg.Future) > 0
}

ghost
requires  len(oldPkt.CurrSeg.Future) > 0
requires  AbsValidateIngressIDConstraint(oldPkt, ingressID)
requires  newPkt == AbsUpdateNonConsDirIngressSegID(oldPkt, ingressID)
ensures   AbsValidateIngressIDConstraint(newPkt, ingressID)
decreases
func AbsValidateIngressIDLemma(oldPkt io.IO_pkt2, newPkt io.IO_pkt2, ingressID option[io.IO_ifs]) {
	reveal AbsValidateIngressIDConstraint(oldPkt, ingressID)
	reveal AbsUpdateNonConsDirIngressSegID(oldPkt, ingressID)
	reveal AbsValidateIngressIDConstraint(newPkt, ingressID)
}

ghost
requires  len(oldPkt.CurrSeg.Future) == 1
requires  oldPkt.LeftSeg != none[io.IO_seg2]
requires  len(get(oldPkt.LeftSeg).Future) > 0
requires  len(get(oldPkt.LeftSeg).History) == 0
requires  AbsValidateIngressIDConstraint(oldPkt, ingressID)
requires  newPkt == AbsDoXover(oldPkt)
ensures   AbsValidateIngressIDConstraintXover(newPkt, ingressID)
decreases
func AbsValidateIngressIDXoverLemma(oldPkt io.IO_pkt2, newPkt io.IO_pkt2, ingressID option[io.IO_ifs]) {
	reveal AbsValidateIngressIDConstraint(oldPkt, ingressID)
	reveal AbsDoXover(oldPkt)
	reveal AbsValidateIngressIDConstraintXover(newPkt, ingressID)
}

ghost
opaque
requires acc(p.scionLayer.Mem(ub), R50)
requires acc(&p.d, R55) && acc(p.d.Mem(), _)
requires acc(&p.ingressID, R55)
decreases
pure func (p *scionPacketProcessor) DstIsLocalIngressID(ub []byte) bool {
	return (unfolding acc(p.scionLayer.Mem(ub), R50) in
		(unfolding acc(p.scionLayer.HeaderMem(ub[slayers.CmnHdrLen:]), R55) in
		p.scionLayer.DstIA) == (unfolding acc(p.d.Mem(), _) in p.d.localIA)) ==> p.ingressID != 0
}

ghost
opaque
requires acc(p.scionLayer.Mem(ub), R50)
requires acc(&p.d, R55) && acc(p.d.Mem(), _)
requires acc(&p.ingressID, R55)
requires acc(sl.AbsSlice_Bytes(ub, 0, len(ub)), R56)
requires slayers.ValidPktMetaHdr(ub)
requires dp.Valid()
decreases
pure func (p *scionPacketProcessor) LastHopLen(ub []byte, dp io.DataPlaneSpec) bool {
	return (unfolding acc(p.scionLayer.Mem(ub), R50) in
		(unfolding acc(p.scionLayer.HeaderMem(ub[slayers.CmnHdrLen:]), R55) in
		p.scionLayer.DstIA) == (unfolding acc(p.d.Mem(), _) in p.d.localIA)) ==>
			len(absPkt(dp, ub).CurrSeg.Future) == 1
}

//TODO: Does not work with --disableNL --unsafeWildcardoptimization
ghost
requires acc(p.scionLayer.Mem(ub), R50)
requires acc(&p.d, R55) && acc(p.d.Mem(), _)
requires acc(&p.ingressID, R55)
requires dp.Valid()
requires acc(sl.AbsSlice_Bytes(ub, 0, len(ub)), R56)
requires slayers.ValidPktMetaHdr(ub)
requires p.DstIsLocalIngressID(ub)
requires p.LastHopLen(ub, dp)
requires (unfolding acc(p.scionLayer.Mem(ub), R50) in
	(unfolding acc(p.scionLayer.HeaderMem(ub[slayers.CmnHdrLen:]), R55) in
	p.scionLayer.DstIA) == (unfolding acc(p.d.Mem(), _) in p.d.localIA))
ensures  acc(p.scionLayer.Mem(ub), R50)
ensures  acc(&p.d, R55) && acc(p.d.Mem(), _)
ensures  acc(&p.ingressID, R55)
ensures  dp.Valid()
ensures  acc(sl.AbsSlice_Bytes(ub, 0, len(ub)), R56)
ensures  slayers.ValidPktMetaHdr(ub)
ensures  p.ingressID != 0
ensures  len(absPkt(dp, ub).CurrSeg.Future) == 1
decreases
func (p* scionPacketProcessor) LocalDstLemma(ub []byte, dp io.DataPlaneSpec) {
	reveal p.DstIsLocalIngressID(ub)
	reveal p.LastHopLen(ub, dp)
}

ghost
requires acc(p.scionLayer.Mem(ub), R55)
requires acc(&p.path, R55) && p.path == p.scionLayer.GetPath(ub)
decreases
pure func (p* scionPacketProcessor) GetIsXoverSpec(ub []byte) bool {
	return let ubPath := p.scionLayer.UBPath(ub) in
		unfolding acc(p.scionLayer.Mem(ub), R55) in
		p.path.GetIsXoverSpec(ubPath)
}

// TODO prove
ghost
requires 0 <= start && start <= end && end <= len(ub)
requires acc(p.scionLayer.Mem(ub), R55)
requires acc(sl.AbsSlice_Bytes(ub, 0, len(ub)), R55)
requires acc(sl.AbsSlice_Bytes(ub[start:end], 0, len(ub[start:end])), R55)
requires acc(&p.path, R55) && acc(p.path.Mem(ub[start:end]), R55)
requires p.path === p.scionLayer.GetPath(ub)
requires dp.Valid()
requires slayers.ValidPktMetaHdr(ub)
requires start == p.scionLayer.PathStartIdx(ub)
requires end == p.scionLayer.PathEndIdx(ub)
requires p.scionLayer.EqAbsHeader(ub)
ensures  acc(sl.AbsSlice_Bytes(ub, 0, len(ub)), R55)
ensures  acc(p.scionLayer.Mem(ub), R55)
ensures  acc(sl.AbsSlice_Bytes(ub[start:end], 0, len(ub[start:end])), R55)
ensures  acc(&p.path, R55) && acc(p.path.Mem(ub[start:end]), R55)
ensures  dp.Valid()
ensures  slayers.ValidPktMetaHdr(ub)
ensures  start == p.scionLayer.PathStartIdx(ub)
ensures  end == p.scionLayer.PathEndIdx(ub)
ensures  scion.validPktMetaHdr(ub[start:end])
ensures  p.path.EqAbsHeader(ub[start:end])
ensures  absPkt(dp, ub) == p.path.absPkt(dp, ub[start:end])
decreases
func (p* scionPacketProcessor) AbsPktToSubSliceAbsPkt(ub []byte, start int, end int, dp io.DataPlaneSpec)
/* {
	reveal validPktMetaHdr(ub)
	reveal p.scionLayer.EqAbsHeader(ub)
	unfold acc(sl.AbsSlice_Bytes(ub, 0, len(ub)), R56)
	unfold acc(sl.AbsSlice_Bytes(ub[start:end], 0, len(ub[start:end])), R56)
	assert reveal scion.validPktMetaHdr(ub[start:end])
	unfold acc(p.scionLayer.Mem(ub), R56)
	assert p.scionLayer.Path.(*scion.Raw).EqAbsHeader(ub[start:end])
	assert p.path.EqAbsHeader(ub[start:end])
	fold acc(p.scionLayer.Mem(ub), R56)
	assert reveal absPkt(dp, ub) == reveal p.path.absPkt(dp, ub[start:end])
	fold acc(sl.AbsSlice_Bytes(ub[start:end], 0, len(ub[start:end])), R56)
	fold acc(sl.AbsSlice_Bytes(ub, 0, len(ub)), R56)
}*/

// TODO prove
ghost
requires 0 <= start && start <= end && end <= len(ub)
requires acc(p.scionLayer.Mem(ub), R55)
requires acc(sl.AbsSlice_Bytes(ub, 0, len(ub)), R55)
requires acc(sl.AbsSlice_Bytes(ub[start:end], 0, len(ub[start:end])), R55)
requires acc(&p.path, R55) && acc(p.path.Mem(ub[start:end]), R55)
requires p.path === p.scionLayer.GetPath(ub)
requires dp.Valid()
requires scion.validPktMetaHdr(ub[start:end])
requires start == p.scionLayer.PathStartIdx(ub)
requires end == p.scionLayer.PathEndIdx(ub)
requires p.path.EqAbsHeader(ub[start:end])
ensures  acc(sl.AbsSlice_Bytes(ub, 0, len(ub)), R55)
ensures  acc(p.scionLayer.Mem(ub), R55)
ensures  acc(sl.AbsSlice_Bytes(ub[start:end], 0, len(ub[start:end])), R55)
ensures  acc(&p.path, R55) && acc(p.path.Mem(ub[start:end]), R55)
ensures  dp.Valid()
ensures  slayers.ValidPktMetaHdr(ub)
ensures  start == p.scionLayer.PathStartIdx(ub)
ensures  end == p.scionLayer.PathEndIdx(ub)
ensures  scion.validPktMetaHdr(ub[start:end])
ensures  p.scionLayer.EqAbsHeader(ub)
ensures  absPkt(dp, ub) == p.path.absPkt(dp, ub[start:end])
decreases
func (p* scionPacketProcessor) SubSliceAbsPktToAbsPkt(ub []byte, start int, end int, dp io.DataPlaneSpec)

ghost
opaque
requires acc(&p.hopField, R55)
requires len(pkt.CurrSeg.Future) > 0
decreases
pure func (p* scionPacketProcessor) EqAbsHopField(pkt io.IO_pkt2) bool {
	return let absHop := p.hopField.ToIO_HF() in
		let currHF := pkt.CurrSeg.Future[0] in
		absHop.InIF2 == currHF.InIF2 &&
		absHop.EgIF2 == currHF.EgIF2 &&
		absHop.HVF == currHF.HVF
}

ghost
opaque
requires acc(&p.infoField, R55)
decreases
pure func (p* scionPacketProcessor) EqAbsInfoField(pkt io.IO_pkt2) bool {
	return let absInf := p.infoField.ToIntermediateAbsInfoField() in
		let currseg := pkt.CurrSeg in
		absInf.AInfo == currseg.AInfo &&
		absInf.UInfo == currseg.UInfo &&
		absInf.ConsDir == currseg.ConsDir &&
		absInf.Peer == currseg.Peer
}