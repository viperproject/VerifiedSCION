// Copyright 2022 ETH Zurich
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// +gobra

package router

import (
	"sync"
	io "verification/io"
)


// TODO: add comment why terminations is assumed
ghost
requires dp.Valid()
requires ingressID != none[io.IO_ifs]
requires len(oldPkt.CurrSeg.Future) > 0
requires ElemWitness(ioSharedArg.IBufY, ingressID, oldPkt)
requires dp.dp2_enter_guard(
			oldPkt,
			oldPkt.CurrSeg,
			io.establishGuardTraversedseg(oldPkt.CurrSeg, !oldPkt.CurrSeg.ConsDir),
			dp.Asid(),
			oldPkt.CurrSeg.Future[0],
			get(ingressID),
			oldPkt.CurrSeg.Future[1:])
requires dp.dp3s_forward(
				io.IO_pkt2(
					io.IO_Packet2{
						io.establishGuardTraversedseg(oldPkt.CurrSeg, !oldPkt.CurrSeg.ConsDir),
						oldPkt.LeftSeg,
						oldPkt.MidSeg,
						oldPkt.RightSeg}),
				newPkt,
				egressID)
preserves acc(ioLock.LockP(), _) && ioLock.LockInv() == SharedInv!< dp, ioSharedArg !>;
ensures ElemWitness(ioSharedArg.OBufY, egressID, newPkt)
decreases _
func AtomicEnter(oldPkt io.IO_pkt2, ingressID option[io.IO_ifs], newPkt io.IO_pkt2, egressID option[io.IO_ifs], ioLock *sync.Mutex, ioSharedArg SharedArg, dp io.DataPlaneSpec) {
    ghost ioLock.Lock()
	unfold SharedInv!< dp, ioSharedArg !>()

	t, s := *ioSharedArg.Place, *ioSharedArg.State

	ApplyElemWitness(s.ibuf, ioSharedArg.IBufY, ingressID, oldPkt)
    ghost pkt_internal := io.IO_val(io.IO_Internal_val1{
                oldPkt,
                get(ingressID),
                newPkt,
                egressID})


	assert dp.dp3s_iospec_bio3s_enter_guard(s, t, pkt_internal)
	unfold dp.dp3s_iospec_ordered(s, t)
	unfold dp.dp3s_iospec_bio3s_enter(s, t)
	io.TriggerBodyIoEnter(pkt_internal)

	tN := io.CBio_IN_bio3s_enter_T(t, pkt_internal)
	io.Enter(t, pkt_internal) //Event

	UpdateElemWitness(s.obuf, ioSharedArg.OBufY, egressID, newPkt)

	ghost *ioSharedArg.State = io.dp3s_add_obuf(s, egressID, newPkt)
	ghost *ioSharedArg.Place = tN

	fold SharedInv!< dp, ioSharedArg !>()
	ghost ioLock.Unlock()
}

// TODO: add comment why terminations is assumed
ghost
requires dp.Valid()
requires ingressID == none[io.IO_ifs]
requires egressID != none[io.IO_ifs]
requires len(oldPkt.CurrSeg.Future) > 0
requires ElemWitness(ioSharedArg.IBufY, ingressID, oldPkt)
requires dp.dp3s_forward_ext(
				oldPkt,
				newPkt,
				get(egressID))
preserves acc(ioLock.LockP(), _) && ioLock.LockInv() == SharedInv!< dp, ioSharedArg !>;
ensures ElemWitness(ioSharedArg.OBufY, egressID, newPkt)
decreases _
func AtomicExit(oldPkt io.IO_pkt2, ingressID option[io.IO_ifs], newPkt io.IO_pkt2, egressID option[io.IO_ifs], ioLock *sync.Mutex, ioSharedArg SharedArg, dp io.DataPlaneSpec) {
    ghost ioLock.Lock()
	unfold SharedInv!< dp, ioSharedArg !>()

	t, s := *ioSharedArg.Place, *ioSharedArg.State

	ApplyElemWitness(s.ibuf, ioSharedArg.IBufY, ingressID, oldPkt)
    ghost pkt_internal := io.IO_val(io.IO_Internal_val2{
                oldPkt,
                newPkt,
                get(egressID)})


	assert dp.dp3s_iospec_bio3s_exit_guard(s, t, pkt_internal)
	unfold dp.dp3s_iospec_ordered(s, t)
	unfold dp.dp3s_iospec_bio3s_exit(s, t)
	io.TriggerBodyIoExit(pkt_internal)

	tN := io.dp3s_iospec_bio3s_exit_T(t, pkt_internal)
	io.Exit(t, pkt_internal) //Event

	UpdateElemWitness(s.obuf, ioSharedArg.OBufY, egressID, newPkt)

	ghost *ioSharedArg.State = io.dp3s_add_obuf(s, egressID, newPkt)
	ghost *ioSharedArg.Place = tN

	fold SharedInv!< dp, ioSharedArg !>()
	ghost ioLock.Unlock()
}


// TODO: add comment why terminations is assumed
ghost
requires dp.Valid()
requires dp.Asid() in dp.Core()
requires oldPkt.LeftSeg != none[io.IO_seg2]
requires oldPkt.CurrSeg.ConsDir == get(oldPkt.LeftSeg).ConsDir
requires len(oldPkt.CurrSeg.Future) > 0
requires len(get(oldPkt.LeftSeg).Future) > 0
requires len(newPkt.CurrSeg.Future) > 0
requires ingressID != none[io.IO_ifs]
requires ElemWitness(ioSharedArg.IBufY, ingressID, oldPkt)
requires dp.xover_core2_link_type(
	oldPkt.CurrSeg.Future[0],
	get(oldPkt.LeftSeg).Future[0],
	dp.Asid(),
	oldPkt.CurrSeg.ConsDir)
requires dp.dp2_xover_common_guard(
	oldPkt,
	oldPkt.CurrSeg,
	get(oldPkt.LeftSeg),
	io.establishGuardTraversedsegInc(oldPkt.CurrSeg, !oldPkt.CurrSeg.ConsDir),
	io.IO_pkt2(io.IO_Packet2{
		get(oldPkt.LeftSeg),
		oldPkt.MidSeg,
		oldPkt.RightSeg,
		some(io.establishGuardTraversedsegInc(oldPkt.CurrSeg, !oldPkt.CurrSeg.ConsDir))}),
	oldPkt.CurrSeg.Future[0],
	get(oldPkt.LeftSeg).Future[0],
	dp.Asid(),
	get(ingressID))
requires dp.dp3s_forward_xover(
	io.IO_pkt2(io.IO_Packet2{
		get(oldPkt.LeftSeg),
		oldPkt.MidSeg,
		oldPkt.RightSeg,
		some(io.establishGuardTraversedsegInc(oldPkt.CurrSeg, !oldPkt.CurrSeg.ConsDir))}),
	newPkt,
	egressID)
preserves acc(ioLock.LockP(), _) && ioLock.LockInv() == SharedInv!< dp, ioSharedArg !>;
ensures ElemWitness(ioSharedArg.OBufY, egressID, newPkt)
decreases _
func AtomicXoverCore(oldPkt io.IO_pkt2, ingressID option[io.IO_ifs], newPkt io.IO_pkt2, egressID option[io.IO_ifs], ioLock *sync.Mutex, ioSharedArg SharedArg, dp io.DataPlaneSpec) {
    ghost ioLock.Lock()
	unfold SharedInv!< dp, ioSharedArg !>()

	t, s := *ioSharedArg.Place, *ioSharedArg.State

	ApplyElemWitness(s.ibuf, ioSharedArg.IBufY, ingressID, oldPkt)
    ghost pkt_internal := io.IO_val(io.IO_Internal_val1{
                oldPkt,
                get(ingressID),
                newPkt,
                egressID})


	assert dp.dp3s_iospec_bio3s_xover_core_guard(s, t, pkt_internal)
	unfold dp.dp3s_iospec_ordered(s, t)
	unfold dp.dp3s_iospec_bio3s_xover_core(s, t)
	io.TriggerBodyIoXoverCore(pkt_internal)

	tN := io.dp3s_iospec_bio3s_xover_core_T(t, pkt_internal)
	io.Xover_core(t, pkt_internal) //Event

	UpdateElemWitness(s.obuf, ioSharedArg.OBufY, egressID, newPkt)

	ghost *ioSharedArg.State = io.dp3s_add_obuf(s, egressID, newPkt)
	ghost *ioSharedArg.Place = tN

	fold SharedInv!< dp, ioSharedArg !>()
	ghost ioLock.Unlock()
}

// TODO: add comment why terminations is assumed
ghost
requires dp.Valid()
requires oldPkt.LeftSeg != none[io.IO_seg2]
requires !oldPkt.CurrSeg.ConsDir
requires get(oldPkt.LeftSeg).ConsDir
requires len(oldPkt.CurrSeg.Future) > 0
requires len(get(oldPkt.LeftSeg).Future) > 0
requires len(newPkt.CurrSeg.Future) > 0
requires ingressID != none[io.IO_ifs]
requires ElemWitness(ioSharedArg.IBufY, ingressID, oldPkt)
requires dp.xover_up2down2_link_type(
	dp.Asid(),
	oldPkt.CurrSeg.Future[0],
	get(oldPkt.LeftSeg).Future[0])
requires dp.dp2_xover_common_guard(
	oldPkt,
	oldPkt.CurrSeg,
	get(oldPkt.LeftSeg),
	io.establishGuardTraversedsegInc(oldPkt.CurrSeg, !oldPkt.CurrSeg.ConsDir),
	io.IO_pkt2(io.IO_Packet2{
		get(oldPkt.LeftSeg),
		oldPkt.MidSeg,
		oldPkt.RightSeg,
		some(io.establishGuardTraversedsegInc(oldPkt.CurrSeg, !oldPkt.CurrSeg.ConsDir))}),
	oldPkt.CurrSeg.Future[0],
	get(oldPkt.LeftSeg).Future[0],
	dp.Asid(),
	get(ingressID))
requires dp.dp3s_forward_xover(
	io.IO_pkt2(io.IO_Packet2{
		get(oldPkt.LeftSeg),
		oldPkt.MidSeg,
		oldPkt.RightSeg,
		some(io.establishGuardTraversedsegInc(oldPkt.CurrSeg, !oldPkt.CurrSeg.ConsDir))}),
	newPkt,
	egressID)
preserves acc(ioLock.LockP(), _) && ioLock.LockInv() == SharedInv!< dp, ioSharedArg !>;
ensures ElemWitness(ioSharedArg.OBufY, egressID, newPkt)
decreases _
func AtomicXoverUp2Down(oldPkt io.IO_pkt2, ingressID option[io.IO_ifs], newPkt io.IO_pkt2, egressID option[io.IO_ifs], ioLock *sync.Mutex, ioSharedArg SharedArg, dp io.DataPlaneSpec) {
    ghost ioLock.Lock()
	unfold SharedInv!< dp, ioSharedArg !>()

	t, s := *ioSharedArg.Place, *ioSharedArg.State

	ApplyElemWitness(s.ibuf, ioSharedArg.IBufY, ingressID, oldPkt)
    ghost pkt_internal := io.IO_val(io.IO_Internal_val1{
                oldPkt,
                get(ingressID),
                newPkt,
                egressID})


	assert dp.dp3s_iospec_bio3s_xover_up2down_guard(s, t, pkt_internal)
	unfold dp.dp3s_iospec_ordered(s, t)
	unfold dp.dp3s_iospec_bio3s_xover_up2down(s, t)
	io.TriggerBodyIoXoverUp2Down(pkt_internal)

	tN := io.dp3s_iospec_bio3s_xover_up2down_T(t, pkt_internal)
	io.Xover_up2down(t, pkt_internal) //Event

	UpdateElemWitness(s.obuf, ioSharedArg.OBufY, egressID, newPkt)

	ghost *ioSharedArg.State = io.dp3s_add_obuf(s, egressID, newPkt)
	ghost *ioSharedArg.Place = tN

	fold SharedInv!< dp, ioSharedArg !>()
	ghost ioLock.Unlock()
}