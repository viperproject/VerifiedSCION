// Copyright 2022 ETH Zurich
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// +gobra

// This file contains the definition of operations that perform the atomic transitions of state
// in the IO spec. They all take a gpointer[gsync.GhostMutex], which acts as a logical invariant, because Gobra
// does not support invariants natively. As such, we can only get access to the invariants if we
// first lock the mutex. Even though all these operations are
// terminating, Gobra cannot currently prove this and thus, we assume termination for all methods
// in this file.

package router

import (
	"sync"
	io "verification/io"
	gsync "verification/utils/ghost_sync"
)

ghost
requires  dp.Valid()
<<<<<<< HEAD
requires  ingressID != none[io.IO_ifs]
=======
requires  ingressID != none[io.Ifs]
>>>>>>> master
requires  len(oldPkt.CurrSeg.Future) > 0
requires  ElemWitness(ioSharedArg.IBufY, ingressID, oldPkt)
requires  dp.dp2_enter_guard(
	oldPkt,
	oldPkt.CurrSeg,
	io.establishGuardTraversedseg(oldPkt.CurrSeg, !oldPkt.CurrSeg.ConsDir),
	dp.Asid(),
	oldPkt.CurrSeg.Future[0],
	get(ingressID),
	oldPkt.CurrSeg.Future[1:])
requires  dp.dp3s_forward(
<<<<<<< HEAD
	io.IO_Packet2 {
=======
	io.Pkt {
>>>>>>> master
		io.establishGuardTraversedseg(oldPkt.CurrSeg, !oldPkt.CurrSeg.ConsDir),
		oldPkt.LeftSeg,
		oldPkt.MidSeg,
		oldPkt.RightSeg,
	},
	newPkt,
	egressID)
preserves acc(ioLock.LockP(), _)
preserves ioLock.LockInv() == SharedInv!< dp, ioSharedArg !>
ensures   ElemWitness(ioSharedArg.OBufY, egressID, newPkt)
decreases _
<<<<<<< HEAD
func AtomicEnter(oldPkt io.IO_pkt2, ingressID option[io.IO_ifs], newPkt io.IO_pkt2, egressID option[io.IO_ifs], ioLock gpointer[gsync.GhostMutex], ioSharedArg SharedArg, dp io.DataPlaneSpec) {
=======
func AtomicEnter(oldPkt io.Pkt, ingressID option[io.Ifs], newPkt io.Pkt, egressID option[io.Ifs], ioLock gpointer[gsync.GhostMutex], ioSharedArg SharedArg, dp io.DataPlaneSpec) {
>>>>>>> master
	ghost ioLock.Lock()
	unfold SharedInv!< dp, ioSharedArg !>()
	t, s := *ioSharedArg.Place, *ioSharedArg.State
	ApplyElemWitness(s.ibuf, ioSharedArg.IBufY, ingressID, oldPkt)
<<<<<<< HEAD
	ghost pkt_internal := io.IO_val(io.IO_Internal_val1{oldPkt, get(ingressID), newPkt, egressID})
=======
	ghost pkt_internal := io.Val(io.ValInternal1{oldPkt, get(ingressID), newPkt, egressID})
>>>>>>> master
	assert dp.dp3s_iospec_bio3s_enter_guard(s, t, pkt_internal)
	unfold dp.dp3s_iospec_ordered(s, t)
	unfold dp.dp3s_iospec_bio3s_enter(s, t)
	io.TriggerBodyIoEnter(pkt_internal)
	tN := io.CBio_IN_bio3s_enter_T(t, pkt_internal)
	io.Enter(t, pkt_internal) //Event
	UpdateElemWitness(s.obuf, ioSharedArg.OBufY, egressID, newPkt)
	ghost *ioSharedArg.State = io.dp3s_add_obuf(s, egressID, newPkt)
	ghost *ioSharedArg.Place = tN
	fold SharedInv!< dp, ioSharedArg !>()
	ghost ioLock.Unlock()
}

ghost
requires  dp.Valid()
<<<<<<< HEAD
requires  ingressID == none[io.IO_ifs]
requires  egressID != none[io.IO_ifs]
=======
requires  ingressID == none[io.Ifs]
requires  egressID != none[io.Ifs]
>>>>>>> master
requires  len(oldPkt.CurrSeg.Future) > 0
requires  ElemWitness(ioSharedArg.IBufY, ingressID, oldPkt)
requires  dp.dp3s_forward_ext(oldPkt, newPkt, get(egressID))
preserves acc(ioLock.LockP(), _)
preserves ioLock.LockInv() == SharedInv!< dp, ioSharedArg !>
ensures   ElemWitness(ioSharedArg.OBufY, egressID, newPkt)
decreases _
<<<<<<< HEAD
func AtomicExit(oldPkt io.IO_pkt2, ingressID option[io.IO_ifs], newPkt io.IO_pkt2, egressID option[io.IO_ifs], ioLock gpointer[gsync.GhostMutex], ioSharedArg SharedArg, dp io.DataPlaneSpec) {
=======
func AtomicExit(oldPkt io.Pkt, ingressID option[io.Ifs], newPkt io.Pkt, egressID option[io.Ifs], ioLock gpointer[gsync.GhostMutex], ioSharedArg SharedArg, dp io.DataPlaneSpec) {
>>>>>>> master
	ghost ioLock.Lock()
	unfold SharedInv!< dp, ioSharedArg !>()
	t, s := *ioSharedArg.Place, *ioSharedArg.State
	ApplyElemWitness(s.ibuf, ioSharedArg.IBufY, ingressID, oldPkt)
<<<<<<< HEAD
	ghost pkt_internal := io.IO_val(io.IO_Internal_val2{oldPkt, newPkt, get(egressID)})
=======
	ghost pkt_internal := io.Val(io.ValInternal2{oldPkt, newPkt, get(egressID)})
>>>>>>> master
	assert dp.dp3s_iospec_bio3s_exit_guard(s, t, pkt_internal)
	unfold dp.dp3s_iospec_ordered(s, t)
	unfold dp.dp3s_iospec_bio3s_exit(s, t)
	io.TriggerBodyIoExit(pkt_internal)
	tN := io.dp3s_iospec_bio3s_exit_T(t, pkt_internal)
	io.Exit(t, pkt_internal) //Event
	UpdateElemWitness(s.obuf, ioSharedArg.OBufY, egressID, newPkt)
	ghost *ioSharedArg.State = io.dp3s_add_obuf(s, egressID, newPkt)
	ghost *ioSharedArg.Place = tN
	fold SharedInv!< dp, ioSharedArg !>()
	ghost ioLock.Unlock()
}

ghost
requires  dp.Valid()
<<<<<<< HEAD
requires  oldPkt.LeftSeg != none[io.IO_seg2]
requires  len(oldPkt.CurrSeg.Future) > 0
requires  len(get(oldPkt.LeftSeg).Future) > 0
requires  ingressID != none[io.IO_ifs]
=======
requires  oldPkt.LeftSeg != none[io.Seg]
requires  len(oldPkt.CurrSeg.Future) > 0
requires  len(get(oldPkt.LeftSeg).Future) > 0
requires  ingressID != none[io.Ifs]
>>>>>>> master
requires  ElemWitness(ioSharedArg.IBufY, ingressID, oldPkt)
requires  dp.dp2_xover_guard(
	oldPkt,
	oldPkt.CurrSeg,
	get(oldPkt.LeftSeg),
	io.establishGuardTraversedsegInc(oldPkt.CurrSeg, !oldPkt.CurrSeg.ConsDir),
<<<<<<< HEAD
	io.IO_Packet2 {
=======
	io.Pkt {
>>>>>>> master
		get(oldPkt.LeftSeg),
		oldPkt.MidSeg,
		oldPkt.RightSeg,
		some(io.establishGuardTraversedsegInc(oldPkt.CurrSeg, !oldPkt.CurrSeg.ConsDir)),
	},
	oldPkt.CurrSeg.Future[0],
	get(oldPkt.LeftSeg).Future[0],
	get(oldPkt.LeftSeg).Future[1:],
	dp.Asid(),
	get(ingressID))
requires  dp.dp3s_forward_xover(
<<<<<<< HEAD
	io.IO_Packet2 {
=======
	io.Pkt {
>>>>>>> master
		get(oldPkt.LeftSeg),
		oldPkt.MidSeg,
		oldPkt.RightSeg,
		some(io.establishGuardTraversedsegInc(oldPkt.CurrSeg, !oldPkt.CurrSeg.ConsDir)),
	},
	newPkt,
	egressID)
preserves acc(ioLock.LockP(), _)
preserves ioLock.LockInv() == SharedInv!< dp, ioSharedArg !>
ensures   ElemWitness(ioSharedArg.OBufY, egressID, newPkt)
decreases _
<<<<<<< HEAD
func AtomicXover(oldPkt io.IO_pkt2, ingressID option[io.IO_ifs], newPkt io.IO_pkt2, egressID option[io.IO_ifs], ioLock gpointer[gsync.GhostMutex], ioSharedArg SharedArg, dp io.DataPlaneSpec) {
=======
func AtomicXover(oldPkt io.Pkt, ingressID option[io.Ifs], newPkt io.Pkt, egressID option[io.Ifs], ioLock gpointer[gsync.GhostMutex], ioSharedArg SharedArg, dp io.DataPlaneSpec) {
>>>>>>> master
	ghost ioLock.Lock()
	unfold SharedInv!< dp, ioSharedArg !>()
	t, s := *ioSharedArg.Place, *ioSharedArg.State
	ApplyElemWitness(s.ibuf, ioSharedArg.IBufY, ingressID, oldPkt)
<<<<<<< HEAD
	ghost pkt_internal := io.IO_val(io.IO_Internal_val1{oldPkt, get(ingressID), newPkt, egressID})
=======
	ghost pkt_internal := io.Val(io.ValInternal1{oldPkt, get(ingressID), newPkt, egressID})
>>>>>>> master
	assert dp.dp3s_iospec_bio3s_xover_guard(s, t, pkt_internal)
	unfold dp.dp3s_iospec_ordered(s, t)
	unfold dp.dp3s_iospec_bio3s_xover(s, t)
	io.TriggerBodyIoXover(pkt_internal)
	tN := io.dp3s_iospec_bio3s_xover_T(t, pkt_internal)
	io.Xover(t, pkt_internal) //Event
	UpdateElemWitness(s.obuf, ioSharedArg.OBufY, egressID, newPkt)
	ghost *ioSharedArg.State = io.dp3s_add_obuf(s, egressID, newPkt)
	ghost *ioSharedArg.Place = tN
	fold SharedInv!< dp, ioSharedArg !>()
	ghost ioLock.Unlock()
}