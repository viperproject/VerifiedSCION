// Copyright 2022 ETH Zurich
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// +gobra

// This file contains the definition of operations that perform the atomic transitions of state
// in the IO spec. They all take a gpointer[gsync.GhostMutex], which acts as a logical invariant, because Gobra
// does not support invariants natively. As such, we can only get access to the invariants if we
// first lock the mutex. Even though all these operations are
// terminating, Gobra cannot currently prove this and thus, we assume termination for all methods
// in this file.

package router

import (
	"sync"
	io "verification/io"
	gsync "verification/utils/ghost_sync"
	"github.com/scionproto/scion/pkg/slayers/path"
)

ghost
requires  dp.Valid()
requires  ingressID != none[io.Ifs]
requires  len(oldPkt.CurrSeg.Future) > 0
requires  ElemWitness(ioSharedArg.IBufY, ingressID, oldPkt)
requires  dp.dp2_enter_guard(
	oldPkt,
	oldPkt.CurrSeg,
	io.establishGuardTraversedseg(oldPkt.CurrSeg, !oldPkt.CurrSeg.ConsDir),
	dp.Asid(),
	oldPkt.CurrSeg.Future[0],
	get(ingressID),
	oldPkt.CurrSeg.Future[1:])
requires  dp.dp3s_forward(
	io.Pkt {
		io.establishGuardTraversedseg(oldPkt.CurrSeg, !oldPkt.CurrSeg.ConsDir),
		oldPkt.LeftSeg,
		oldPkt.MidSeg,
		oldPkt.RightSeg,
	},
	newPkt,
	egressID)
preserves acc(ioLock.LockP(), _)
preserves ioLock.LockInv() == SharedInv!< dp, ioSharedArg !>
ensures   ElemWitness(ioSharedArg.OBufY, egressID, newPkt)
decreases _
func AtomicEnter(oldPkt io.Pkt, ingressID option[io.Ifs], newPkt io.Pkt, egressID option[io.Ifs], ioLock gpointer[gsync.GhostMutex], ioSharedArg SharedArg, dp io.DataPlaneSpec) {
	ghost ioLock.Lock()
	unfold SharedInv!< dp, ioSharedArg !>()
	t, s := *ioSharedArg.Place, *ioSharedArg.State
	ApplyElemWitness(s.ibuf, ioSharedArg.IBufY, ingressID, oldPkt)
	ghost pkt_internal := io.Val(io.ValInternal1{oldPkt, get(ingressID), newPkt, egressID})
	assert dp.dp3s_iospec_bio3s_enter_guard(s, t, pkt_internal)
	unfold dp.dp3s_iospec_ordered(s, t)
	unfold dp.dp3s_iospec_bio3s_enter(s, t)
	io.TriggerBodyIoEnter(pkt_internal)
	tN := io.CBio_IN_bio3s_enter_T(t, pkt_internal)
	io.Enter(t, pkt_internal) //Event
	UpdateElemWitness(s.obuf, ioSharedArg.OBufY, egressID, newPkt)
	ghost *ioSharedArg.State = io.dp3s_add_obuf(s, egressID, newPkt)
	ghost *ioSharedArg.Place = tN
	fold SharedInv!< dp, ioSharedArg !>()
	ghost ioLock.Unlock()
}

ghost
requires  dp.Valid()
requires  ingressID == none[io.Ifs]
requires  egressID != none[io.Ifs]
requires  len(oldPkt.CurrSeg.Future) > 0
requires  ElemWitness(ioSharedArg.IBufY, ingressID, oldPkt)
requires  dp.dp3s_forward_ext(oldPkt, newPkt, get(egressID))
preserves acc(ioLock.LockP(), _)
preserves ioLock.LockInv() == SharedInv!< dp, ioSharedArg !>
ensures   ElemWitness(ioSharedArg.OBufY, egressID, newPkt)
decreases _
func AtomicExit(oldPkt io.Pkt, ingressID option[io.Ifs], newPkt io.Pkt, egressID option[io.Ifs], ioLock gpointer[gsync.GhostMutex], ioSharedArg SharedArg, dp io.DataPlaneSpec) {
	ghost ioLock.Lock()
	unfold SharedInv!< dp, ioSharedArg !>()
	t, s := *ioSharedArg.Place, *ioSharedArg.State
	ApplyElemWitness(s.ibuf, ioSharedArg.IBufY, ingressID, oldPkt)
	ghost pkt_internal := io.Val(io.ValInternal2{oldPkt, newPkt, get(egressID)})
	assert dp.dp3s_iospec_bio3s_exit_guard(s, t, pkt_internal)
	unfold dp.dp3s_iospec_ordered(s, t)
	unfold dp.dp3s_iospec_bio3s_exit(s, t)
	io.TriggerBodyIoExit(pkt_internal)
	tN := io.dp3s_iospec_bio3s_exit_T(t, pkt_internal)
	io.Exit(t, pkt_internal) //Event
	UpdateElemWitness(s.obuf, ioSharedArg.OBufY, egressID, newPkt)
	ghost *ioSharedArg.State = io.dp3s_add_obuf(s, egressID, newPkt)
	ghost *ioSharedArg.Place = tN
	fold SharedInv!< dp, ioSharedArg !>()
	ghost ioLock.Unlock()
}

ghost
requires  dp.Valid()
requires  oldPkt.LeftSeg != none[io.Seg]
requires  len(oldPkt.CurrSeg.Future) > 0
requires  len(get(oldPkt.LeftSeg).Future) > 0
requires  ingressID != none[io.Ifs]
requires  ElemWitness(ioSharedArg.IBufY, ingressID, oldPkt)
requires  dp.dp2_xover_guard(
	oldPkt,
	oldPkt.CurrSeg,
	get(oldPkt.LeftSeg),
	io.establishGuardTraversedsegInc(oldPkt.CurrSeg, !oldPkt.CurrSeg.ConsDir),
	io.Pkt {
		get(oldPkt.LeftSeg),
		oldPkt.MidSeg,
		oldPkt.RightSeg,
		some(io.establishGuardTraversedsegInc(oldPkt.CurrSeg, !oldPkt.CurrSeg.ConsDir)),
	},
	oldPkt.CurrSeg.Future[0],
	get(oldPkt.LeftSeg).Future[0],
	get(oldPkt.LeftSeg).Future[1:],
	dp.Asid(),
	get(ingressID))
requires  dp.dp3s_forward_xover(
	io.Pkt {
		get(oldPkt.LeftSeg),
		oldPkt.MidSeg,
		oldPkt.RightSeg,
		some(io.establishGuardTraversedsegInc(oldPkt.CurrSeg, !oldPkt.CurrSeg.ConsDir)),
	},
	newPkt,
	egressID)
preserves acc(ioLock.LockP(), _)
preserves ioLock.LockInv() == SharedInv!< dp, ioSharedArg !>
ensures   ElemWitness(ioSharedArg.OBufY, egressID, newPkt)
decreases _
func AtomicXover(oldPkt io.Pkt, ingressID option[io.Ifs], newPkt io.Pkt, egressID option[io.Ifs], ioLock gpointer[gsync.GhostMutex], ioSharedArg SharedArg, dp io.DataPlaneSpec) {
	ghost ioLock.Lock()
	unfold SharedInv!< dp, ioSharedArg !>()
	t, s := *ioSharedArg.Place, *ioSharedArg.State
	ApplyElemWitness(s.ibuf, ioSharedArg.IBufY, ingressID, oldPkt)
	ghost pkt_internal := io.Val(io.ValInternal1{oldPkt, get(ingressID), newPkt, egressID})
	assert dp.dp3s_iospec_bio3s_xover_guard(s, t, pkt_internal)
	unfold dp.dp3s_iospec_ordered(s, t)
	unfold dp.dp3s_iospec_bio3s_xover(s, t)
	io.TriggerBodyIoXover(pkt_internal)
	tN := io.dp3s_iospec_bio3s_xover_T(t, pkt_internal)
	io.Xover(t, pkt_internal) //Event
	UpdateElemWitness(s.obuf, ioSharedArg.OBufY, egressID, newPkt)
	ghost *ioSharedArg.State = io.dp3s_add_obuf(s, egressID, newPkt)
	ghost *ioSharedArg.Place = tN
	fold SharedInv!< dp, ioSharedArg !>()
	ghost ioLock.Unlock()
}

ghost
requires  low(beta) && low(ts) && low(inif) && low(egif)
requires  path.AbsMac(sigma) == io.nextMsgtermSpec(dp.Asid(), inif, egif, ts, beta)
preserves acc(ioLock.LockP(), _)
preserves ioLock.LockInv() == SharedInv!< dp, ioSharedArg !>
ensures   forall i int :: { sigma[i] } 0 <= i && i < len(sigma) ==> low(sigma[i])
decreases _
func AtomicDecl(beta set[io.MsgTerm], ts uint, inif, egif option[io.Ifs], sigma [path.MacLen]byte, ioLock gpointer[gsync.GhostMutex], ioSharedArg SharedArg, dp io.DataPlaneSpec) {
	ghost ioLock.Lock()
	unfold SharedInv!< dp, ioSharedArg !>()

	t, s := *ioSharedArg.Place, *ioSharedArg.State
	ghost decl_val := io.Val(io.IO_decl_val{beta, ts, inif, egif, sigma})

	// TODO: Remove this assumption
	assume path.AbsMac(sigma) == io.AbsMac(sigma)
	assert dp.dp4s_iospec_bio4s_decl_guard(s, t, decl_val)
	unfold dp.dp3s_iospec_ordered(s, t)
	unfold dp.dp4s_iospec_bio4s_decl(s, t)

	io.TriggerBodyIoDecl(decl_val)
	tN := io.dp4s_iospec_bio4s_decl_T(t, decl_val)
	io.Decl(t, decl_val)
	ghost *ioSharedArg.Place = tN

	fold SharedInv!< dp, ioSharedArg !>()
	ghost ioLock.Unlock()
}